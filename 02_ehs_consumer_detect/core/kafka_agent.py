import json
from confluent_kafka import Consumer, KafkaException, TopicPartition
import sys
import time
import logging
from typing import Callable, Dict, Tuple, List, Any

# å…¨å±€é…ç½®ï¼Œè¯·æ ¹æ®æ‚¨çš„ç¯å¢ƒä¿®æ”¹
KAFKA_BOOTSTRAP_SERVERS = "kafka:29092"
KAFKA_CONSUMER_GROUP_ID = "my-confluent-group"

# è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

BatchProcessFun = Callable[[str, int, List[Tuple[int, Any]]], None]
#class ConfluentKafkaAgent:
class KafkaAgent:
    """
    ä½¿ç”¨ confluent_kafka åº“çš„æ¶ˆè´¹è€…ä»£ç†ï¼Œæ¨èä½¿ç”¨ Kafka/Zookeeper ç®¡ç† offsetã€‚
    """
    def __init__(self, topics, model, device,group_id,bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS, auto_offset_reset='earliest', enable_auto_commit=True):
        if isinstance(topics, str):
            topics = [topics]
        self.topics = topics
        #self.production_line = production_line
        # å°†æ‰¹å¤„ç†çŠ¶æ€å®šä¹‰ä¸ºå®ä¾‹å±æ€§
        self._message_buffer: Dict[Tuple[str, int], List[Tuple[int, Any]]] = {}
        self._batch_start_time: Dict[Tuple[str, int], float] = {}
        # âœ… å­˜å‚¨æ¨¡å‹å’Œè®¾å¤‡ä½œä¸ºå®ä¾‹å±æ€§
        self.model = model
        self.device = device
        
        # 1. é…ç½® Consumer
        conf = {
            'bootstrap.servers': bootstrap_servers,
            'group.id': group_id,
            'auto.offset.reset': auto_offset_reset,  # 'earliest' æˆ– 'latest'
            'enable.auto.commit': enable_auto_commit,
            'session.timeout.ms': 6000,
            'max.poll.interval.ms': 300000, # æ¶ˆæ¯å¤„ç†æ—¶é—´è¿‡é•¿æ—¶è®¾ç½®æ­¤é¡¹
            # 'default.topic.config': {'auto.offset.reset': 'earliest'} # å¯é€‰
        }

        # 2. åˆå§‹åŒ– Consumer
        self.consumer = Consumer(conf)
        
        # 3. è®¢é˜… Topic
        self.consumer.subscribe(self.topics)
        print(f"Consumer subscribed to topics: {self.topics}")

    def _handle_message_error(self, msg: Any) -> bool:
        """
        å¤„ç† Kafka æ¶ˆæ¯é”™è¯¯ã€‚
        è¿”å› True è¡¨ç¤ºé‡åˆ°è‡´å‘½é”™è¯¯éœ€è¦åœæ­¢ï¼Œè¿”å› False è¡¨ç¤ºå¯ä»¥ç»§ç»­ã€‚
        """
        if msg.error():
            error = msg.error()
            if error.fatal():
                logger.error(f"Fatal Consumer error, stopping: {error}")
                return True
            else:
                logger.warning(f"Consumer error (recoverable): {error}")
                return False
        return False

    def _decode_and_parse_message(self, msg: Any) -> Tuple[str, int, int, Any]:
        """
        è§£ç å’Œååºåˆ—åŒ–æ¶ˆæ¯å€¼ï¼Œå¹¶æå–å…ƒæ•°æ®ã€‚
        æˆåŠŸè¿”å› (topic, partition, offset, value_obj)ï¼Œå¤±è´¥æŠ›å‡ºå¼‚å¸¸ã€‚
        """
        topic = msg.topic()
        partition = msg.partition()
        offset = msg.offset()
        
        value_bytes = msg.value()
        if value_bytes is None:
             raise ValueError("Message value is None.")
        
        try:
            value_str = value_bytes.decode('utf-8')
            value = json.loads(value_str)
            return topic, partition, offset, value
        except (UnicodeDecodeError, json.JSONDecodeError) as e:
            logger.warning(f"Failed to decode/parse message for {topic}/{partition} @ {offset}. Error: {e}")
            raise # ç»Ÿä¸€å‘ä¸ŠæŠ›å‡ºï¼Œç”±è°ƒç”¨æ–¹å†³å®šè·³è¿‡
        except Exception as e:
            logger.error(f"Unknown error processing message for {topic}/{partition} @ {offset}: {e}")
            raise

    # ğŸš€ ä¼˜åŒ–ç‚¹ï¼šå°† flush_batch æ”¹ä¸ºç±»æ–¹æ³•
    def _flush_batch(self, topic: str, partition: int, process_fun: BatchProcessFun):
        """å¯¹ç‰¹å®š Topic-Partition çš„æ¶ˆæ¯æ‰§è¡Œæ‰¹é‡å¤„ç†å¹¶æ¸…ç©ºç¼“å†²åŒºã€‚"""
        key = (topic, partition)
        
        # ç›´æ¥è®¿é—®å®ä¾‹å±æ€§
        batch_data = self._message_buffer.pop(key, [])
        if not batch_data:
            return
        
        # æ¸…é™¤å¼€å§‹æ—¶é—´è®°å½•
        self._batch_start_time.pop(key, None)
        
        # æ‰¹é‡è°ƒç”¨ä¸šåŠ¡å¤„ç†å‡½æ•°
        process_fun(topic, partition, batch_data, self.model, self.device)
        
        logger.debug(f"Flushed batch for {topic}/{partition}. Size: {len(batch_data)}")


    def run(self, process_fun: Callable):
        """
        å¯åŠ¨å•æ¶ˆæ¯æ¶ˆè´¹å¾ªç¯ï¼ˆä»£ç ä¸ä¸Šä¸€ä¸ªä¼˜åŒ–ç‰ˆæœ¬ä¸€è‡´ï¼‰ã€‚
        """
        logger.info("Starting single message consumer loop...")
        try:
            while True:
                msg = self.consumer.poll(1.0)
                
                if msg is None:
                    continue

                if self._handle_message_error(msg):
                    break
                
                try:
                    topic_name, partition_id, _, value = self._decode_and_parse_message(msg)
                except Exception:
                    continue
                
                process_fun(topic_name, partition_id, [value])
                
        except KeyboardInterrupt:
            logger.info("Consumer stopped by user (KeyboardInterrupt).")
        
        finally:
            logger.info("Closing consumer...")
            self.consumer.close()


    def run_batch(self, process_fun: BatchProcessFun, batch_size: int = 100, batch_timeout_ms: int = 3000):
        """
        å¯åŠ¨æ‰¹é‡æ¶ˆè´¹å¾ªç¯ã€‚
        """
        # æ¸…ç©ºçŠ¶æ€ï¼Œç¡®ä¿ run_batch çš„æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯æ–°çš„æ‰¹å¤„ç†å‘¨æœŸ
        self._message_buffer.clear()
        self._batch_start_time.clear()

        timeout_seconds = batch_timeout_ms / 1000.0
        
        logger.info("Starting batch consumer loop...")
        try:
            while True:
                msg = self.consumer.poll(0.1) 
                
                # --- A. æ£€æŸ¥å¹¶å¤„ç†æ”¶åˆ°çš„æ¶ˆæ¯ ---
                if msg is not None:
                    
                    if self._handle_message_error(msg):
                        break
                    
                    # æ¶ˆæ¯å¤„ç† (è§£ç å’Œååºåˆ—åŒ–)
                    try:
                        topic, partition, offset, value = self._decode_and_parse_message(msg)
                    except Exception:
                        continue 

                    key = (topic, partition)

                    # 2. æ¶ˆæ¯å…¥ç¼“å†²åŒº
                    if key not in self._message_buffer:
                        self._message_buffer[key] = []
                        self._batch_start_time[key] = time.time()
                    
                    #self._message_buffer[key].append((offset, value))
                    self._message_buffer[key].append(value)
                    
                    # 3. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹æ¬¡å¤§å°
                    if len(self._message_buffer[key]) >= batch_size:
                        # è°ƒç”¨ç±»æ–¹æ³•ï¼Œä¼ å…¥ process_fun
                        self._flush_batch(topic, partition, process_fun)
                
                # --- B. æ£€æŸ¥å¹¶å¤„ç†æ‰¹æ¬¡è¶…æ—¶ ---
                
                keys_to_check = list(self._batch_start_time.keys())
                current_time = time.time()
                
                for topic, partition in keys_to_check:
                    key = (topic, partition)
                    start_time = self._batch_start_time.get(key)
                    
                    # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦å­˜åœ¨ï¼ˆç¡®ä¿è®¡æ—¶å™¨å’Œæ•°æ®åŒ¹é…ï¼‰ä¸”è®¡æ—¶æœ‰æ•ˆ
                    if key in self._message_buffer and start_time is not None:
                        if current_time - start_time >= timeout_seconds:
                            # è°ƒç”¨ç±»æ–¹æ³•ï¼Œä¼ å…¥ process_fun
                            self._flush_batch(topic, partition, process_fun)
                            
        except KeyboardInterrupt:
            logger.info("Consumer stopped by user (KeyboardInterrupt).")
        
        finally:
            # 5. å…³é—­ Consumerï¼Œé€€å‡ºå‰åˆ·æ–°æ‰€æœ‰å‰©ä½™ç¼“å†²åŒºä¸­çš„æ•°æ®
            logger.info("Flushing remaining batches before closing...")
            # éå† message_buffer çš„é”®ï¼Œç¡®ä¿æ¸…ç©ºæ‰€æœ‰æ•°æ®
            for topic, partition in list(self._message_buffer.keys()):
                # ä¼ å…¥ process_fun
                self._flush_batch(topic, partition, process_fun) 
                
            logger.info("Closing consumer...")
            self.consumer.close()

# --- ç¤ºä¾‹ä¸šåŠ¡å¤„ç†å‡½æ•° ---
def process_data(topic_name,partition_id,data):
    """ä¸€ä¸ªç®€å•çš„å¤„ç†å‡½æ•°ï¼Œæ‰“å°æ¥æ”¶åˆ°çš„æ•°æ®"""
    print(f"Received data: {data}")
    # time.sleep(0.1) # æ¨¡æ‹Ÿå¤„ç†è€—æ—¶

# --- è¿è¡Œç¤ºä¾‹ ---
# if __name__ == '__main__':
#     # ç¤ºä¾‹ï¼šä½¿ç”¨æ‰‹åŠ¨æäº¤ offset
#     agent = ConfluentKafkaAgent(
#         topics=["your-topic-name"],
#         group_id=KAFKA_CONSUMER_GROUP_ID,
#         bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#         enable_auto_commit=True # æ¨èä½¿ç”¨è‡ªåŠ¨æäº¤ï¼Œé™¤éæ‚¨éœ€è¦â€œæ°å¥½ä¸€æ¬¡â€è¯­ä¹‰
#     )
#     agent.run(process_data)